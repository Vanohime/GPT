# GPT
This project is a reimplementation of the GPT-2 architecture, faithfully following [the original paper] (https://arxiv.org/abs/1706.03762). The model was trained on the FineWeb dataset, using approximately 10 billion tokens. For evaluation, the HellaSwag benchmark was used, where this implementation outperformed the original GPT-2 by 10% in validation accuracy.
